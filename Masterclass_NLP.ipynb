{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This comprehensive notebook teaches you **Natural Language Processing fundamentals** through a hands-on **Sentiment Analysis Project**. You'll learn both the theory and practical implementation of core NLP concepts."
      ],
      "metadata": {
        "id": "hMWXw7ILy3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Introduction to NLP\n",
        "\n",
        "### What is Natural Language Processing?\n",
        "\n",
        "**Definition**: Natural Language Processing is a branch of artificial intelligence that focuses on enabling computers to understand, interpret, analyze, and generate human language in a meaningful and useful way.\n",
        "\n",
        "### Two Main Components\n",
        "\n",
        "#### a) Natural Language Understanding (NLU)\n",
        "- **Goal**: Make computers understand human language\n",
        "- **Tasks**: Reading comprehension, question answering, named entity recognition\n",
        "- **Challenge**: Ambiguity, context, idioms, sarcasm\n",
        "\n",
        "#### b) Natural Language Generation (NLG)\n",
        "- **Goal**: Enable computers to generate human-like text\n",
        "- **Tasks**: Text summarization, machine translation, chatbot responses\n",
        "- **Challenge**: Grammatical correctness, coherence, relevance\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "| Application | Use Case | Example |\n",
        "|---|---|---|\n",
        "| **Sentiment Analysis** | Determine emotional tone of text | Reviews, social media monitoring |\n",
        "| **Text Classification** | Categorize documents | Email spam detection, news categorization |\n",
        "| **Named Entity Recognition** | Extract entities from text | Identifying person names, locations, organizations |\n",
        "| **Machine Translation** | Translate between languages | Google Translate |\n",
        "| **Question Answering** | Answer questions about documents | Chatbots, search engines |\n",
        "| **Text Summarization** | Create concise summaries | News summaries, document abstracts |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IWTe41VG0HJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "### Required Libraries\n",
        "\n",
        "We'll use these popular Python NLP libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "Dg5jRBkp1GNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk textblob spacy pandas numpy matplotlib scikit-learn\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCc2qq6H1lZT",
        "outputId": "fbda9e05-f4b4-4f83-d421-6e21be7f794b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import All Libraries"
      ],
      "metadata": {
        "id": "M6HBkRM11Tyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')  # Tokenization\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')  # Stop words\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagging\n",
        "nltk.download('wordnet')  # Lemmatization\n",
        "nltk.download('maxent_ne_chunker')  # Named Entity Recognition\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "print(\" All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZgq7HVc19Wp",
        "outputId": "280e2feb-3d72-4044-9205-ade7dc709989"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All libraries imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Core NLP Concepts"
      ],
      "metadata": {
        "id": "r--97sKp2O6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Tokenization\n",
        "\n",
        "**Definition**: Breaking down text into smaller meaningful units called tokens (words, subwords, or characters)\n",
        "\n",
        "**Why It Matters**: Most NLP algorithms operate on token-level basis, not raw text\n",
        "\n",
        "**Types of Tokenization**:\n",
        "- **Word Tokenization**: Split into words\n",
        "- **Sentence Tokenization**: Split into sentences\n",
        "- **Subword Tokenization**: Split into subwords (e.g., \"unhappy\" â†’ \"un\" + \"happy\")\n",
        "\n",
        "\n",
        "**Theory**: Tokenization is the foundation of NLP. Different languages require different tokenization strategies (e.g., Chinese has no word boundaries, requiring more sophisticated techniques).\n",
        "\n",
        "---\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "-AsSdOsY2TB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "text = \"Natural Language Processing is amazing! It helps machines understand text.\"\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n",
        "# Output: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'It', ...]\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "# Output: ['Natural Language Processing is amazing!', 'It helps machines understand text.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMBgurOB6RQN",
        "outputId": "0c6d9932-9ba3-4bdc-c86a-4d676e03d9ae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'It', 'helps', 'machines', 'understand', 'text', '.']\n",
            "Sentences: ['Natural Language Processing is amazing!', 'It helps machines understand text.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Stop Words\n",
        "\n",
        "**Definition**: Common words in a language that carry minimal meaningful information (the, is, and, or, etc.)\n",
        "\n",
        "**Why Remove Them**: Stop words can obscure the main meaning and increase computational cost\n",
        "\n",
        "\n",
        "**Caution**: Be careful when removing stop words from sentiment analysis (e.g., \"not good\" â†’ \"good\" loses crucial information)\n",
        "\n",
        "---\n",
        "\n",
        "**Example**:"
      ],
      "metadata": {
        "id": "9NmMTkAc2XPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Sample stop words:\", list(stop_words)[:10])\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "print(\"Original:\", tokens)\n",
        "print(\"Filtered:\", filtered_tokens)\n",
        "# Output - Filtered: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6scdFchC7TNM",
        "outputId": "82ac1f16-857f-4086-bafe-8ddcae4226dc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample stop words: [\"he'll\", 'out', 'each', 'from', 'can', 'further', \"needn't\", \"i'm\", \"mustn't\", 'over']\n",
            "Original: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Filtered: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Lowercasing & Cleaning\n",
        "\n",
        "**Definition**: Normalize text by converting to lowercase and removing special characters\n",
        "\n",
        "**Why It Matters**:\n",
        "- \"Apple\", \"apple\", and \"APPLE\" should be treated as the same token\n",
        "- Numbers and special characters often don't add semantic value\n",
        "\n",
        "**Example**:\n"
      ],
      "metadata": {
        "id": "jLz5-BtO2bZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "raw_text = \"Check this out!!! Visit https://example.com #NLP @Python123\"\n",
        "cleaned = clean_text(raw_text)\n",
        "print(\"Original:\", raw_text)\n",
        "print(\"Cleaned:\", cleaned)\n",
        "# Output - Cleaned: \"check this out visit example nlp python\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8essHEh7oTk",
        "outputId": "1d06226b-2cc8-4ec4-b606-307b6c72dc7d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Check this out!!! Visit https://example.com #NLP @Python123\n",
            "Cleaned: check this out visit nlp python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Preprocessing Pipeline\n",
        "\n",
        "**Definition**: A systematic approach to prepare raw text for analysis\n",
        "\n",
        "**Steps in Order**:\n",
        "1. Remove HTML/URLs\n",
        "2. Convert to lowercase\n",
        "3. Remove special characters\n",
        "4. Tokenization\n",
        "5. Remove stop words\n",
        "6. Lemmatization/Stemming\n",
        "\n",
        "**Example**:"
      ],
      "metadata": {
        "id": "56S0SC772evL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    # 1. Remove URLs and special patterns\n",
        "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
        "\n",
        "    # 2. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # 4. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 5. Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # 6. Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Test\n",
        "sample = \"I'm absolutely LOVING this amazing NLP course! Check it out: https://akhil.com #ML\"\n",
        "result = preprocess_text(sample)\n",
        "print(\"Input:\", sample)\n",
        "print(\"Output:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omhYuqpl8BEe",
        "outputId": "2b04593a-55b5-416e-a3b8-7324d1761fe3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I'm absolutely LOVING this amazing NLP course! Check it out: https://akhil.com #ML\n",
            "Output: im absolutely loving amazing nlp course check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Tokenization In Detail\n",
        "\n",
        "### Different Tokenization Methods"
      ],
      "metadata": {
        "id": "if_9VqAR2i8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "\n",
        "text = \"Dr. Smith works at Google Inc. He loves NLP!\"\n",
        "\n",
        "# Method 1: NLTK Word Tokenization\n",
        "nltk_tokens = word_tokenize(text)\n",
        "print(\"NLTK:\", nltk_tokens)\n",
        "\n",
        "# Method 2: NLTK Sentence Tokenization\n",
        "nltk_sents = sent_tokenize(text)\n",
        "print(\"Sentences:\", nltk_sents)\n",
        "\n",
        "# Method 3: spaCy Tokenization (More sophisticated)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"spaCy:\", spacy_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd2YgqA_8TSv",
        "outputId": "16a2fa87-163f-4ba3-f2be-a268486121ce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK: ['Dr.', 'Smith', 'works', 'at', 'Google', 'Inc', '.', 'He', 'loves', 'NLP', '!']\n",
            "Sentences: ['Dr. Smith works at Google Inc.', 'He loves NLP!']\n",
            "spaCy: ['Dr.', 'Smith', 'works', 'at', 'Google', 'Inc.', 'He', 'loves', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy advantages:\n",
        "- Handles abbreviations better (\"Dr.\" is one token, not two)\n",
        "- Provides additional linguistic information\n",
        "- More accurate for complex text"
      ],
      "metadata": {
        "id": "779xgxDn8Y8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Normalization & Text Cleaning\n",
        "\n",
        "### Techniques Explained\n",
        "\n",
        "**1. Case Normalization**\n",
        "\n"
      ],
      "metadata": {
        "id": "T_d6k7SV2mwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Machine Learning and MACHINE learning\"\n",
        "normalized = text.lower()\n",
        "print(normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hscVYzLB9Aej",
        "outputId": "185b5fff-52bb-4b5e-e17f-587ec73a344f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine learning and machine learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. Punctuation Removal**"
      ],
      "metadata": {
        "id": "UVFfO7NV89io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, world! How are you?\"\n",
        "no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-TKbdDu9H0Z",
        "outputId": "b6b0b76a-5fc7-47d8-880f-578331b20d59"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world How are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**3. Number Handling**"
      ],
      "metadata": {
        "id": "Sap8hDBN9Mfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Remove all numbers\n",
        "text = \"I have 2 cats and 3 dogs\"\n",
        "no_nums = re.sub(r'\\d+', '', text)\n",
        "print(no_nums)\n",
        "\n",
        "# Option B: Replace with placeholder\n",
        "text_placeholder = re.sub(r'\\d+', '[NUM]', text)\n",
        "print(text_placeholder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP0iXCSJ9PFF",
        "outputId": "1ad8cb09-e43a-4bd2-e6a6-50857030ed78"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have  cats and  dogs\n",
            "I have [NUM] cats and [NUM] dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Whitespace Normalization**"
      ],
      "metadata": {
        "id": "-LYAuM_Y9XaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello    world    from    NLP\"\n",
        "normalized = ' '.join(text.split())\n",
        "print(normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlY6Lwss9TfK",
        "outputId": "478f48b5-bcfb-47dd-d347-8efcdddb6e70"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world from NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Lemmatization & Stemming\n",
        "\n",
        "### Lemmatization vs Stemming\n",
        "\n",
        "| Aspect | Lemmatization | Stemming |\n",
        "|--------|---------------|----------|\n",
        "| **Method** | Uses dictionary & grammar rules | Rule-based heuristic |\n",
        "| **Output** | Actual dictionary word | Word root (may not be real word) |\n",
        "| **Accuracy** | Higher | Lower |\n",
        "| **Speed** | Slower | Faster |\n",
        "| **Use Case** | Precision needed | Speed priority |\n",
        "\n",
        "**Lemmatization Example**:\n"
      ],
      "metadata": {
        "id": "fhCvDYlj2s05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"runs\", \"runner\", \"cars\", \"was\", \"being\"]\n",
        "\n",
        "for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:15} â†’ {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvAQWjIj9tPV",
        "outputId": "fb95d7ca-c2fe-4f7d-d860-0c0e44580941"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running         â†’ running\n",
            "ran             â†’ ran\n",
            "runs            â†’ run\n",
            "runner          â†’ runner\n",
            "cars            â†’ car\n",
            "was             â†’ wa\n",
            "being           â†’ being\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution - POS Tagging Help**:"
      ],
      "metadata": {
        "id": "FLCGxKVx_qGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "text = \"The dogs were running in the park\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = []\n",
        "for word, pos in pos_tags:\n",
        "    if pos.startswith('V'):  # Verb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "    lemmatized.append(lemma)\n",
        "\n",
        "print(' '.join(lemmatized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYT4wtUT_rCj",
        "outputId": "7e93987e-4610-4373-d06c-c7a9f634d755"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('dogs', 'NNS'), ('were', 'VBD'), ('running', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN')]\n",
            "The dog be run in the park\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming Example**:"
      ],
      "metadata": {
        "id": "7KL9VhPR_188"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"runs\", \"runner\", \"connection\", \"connecting\"]\n",
        "\n",
        "for word in words:\n",
        "    stem = stemmer.stem(word)\n",
        "    print(f\"{word:15} â†’ {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO87x82F_4iy",
        "outputId": "2124245d-b0a7-4ff8-f396-559d5875ef5a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running         â†’ run\n",
            "ran             â†’ ran\n",
            "runs            â†’ run\n",
            "runner          â†’ runner\n",
            "connection      â†’ connect\n",
            "connecting      â†’ connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Sentiment Analysis Project {#project}\n",
        "\n",
        "### Project Overview\n",
        "\n",
        "**Objective**: Build a sentiment analysis model to classify text as Positive, Negative, or Neutral\n",
        "\n",
        "**Dataset**: Reviews from various sources\n",
        "\n",
        "**Skills Practiced**:\n",
        "- Text preprocessing\n",
        "- Feature extraction\n",
        "- Machine learning classification\n",
        "- Model evaluation"
      ],
      "metadata": {
        "id": "eiusn00p_hCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Create Sample Dataset"
      ],
      "metadata": {
        "id": "8hwGXyVh_bu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample dataset of reviews\n",
        "reviews = [\n",
        "    \"I absolutely love this product! Best purchase ever!\",\n",
        "    \"Terrible quality. Completely disappointed with my order.\",\n",
        "    \"It's okay. Nothing special but does the job.\",\n",
        "    \"Amazing customer service and fast delivery!\",\n",
        "    \"Waste of money. Product broke after one day.\",\n",
        "    \"Not bad. I'd recommend it to friends.\",\n",
        "    \"Fantastic experience! Will definitely buy again!\",\n",
        "    \"Awful. Poor quality and worse than expected.\",\n",
        "    \"Good value for money. Pretty satisfied.\",\n",
        "    \"Horrible experience. Never buying again.\"\n",
        "]\n",
        "\n",
        "sentiments = [\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"neutral\",\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"neutral\",\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"positive\",\n",
        "    \"negative\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'review': reviews,\n",
        "    'sentiment': sentiments\n",
        "})\n",
        "\n",
        "print(df)\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXyq1xik_YFz",
        "outputId": "2bd07256-b004-428a-9d69-8f602f279c16"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  I absolutely love this product! Best purchase ...  positive\n",
            "1  Terrible quality. Completely disappointed with...  negative\n",
            "2       It's okay. Nothing special but does the job.   neutral\n",
            "3        Amazing customer service and fast delivery!  positive\n",
            "4       Waste of money. Product broke after one day.  negative\n",
            "5              Not bad. I'd recommend it to friends.   neutral\n",
            "6   Fantastic experience! Will definitely buy again!  positive\n",
            "7       Awful. Poor quality and worse than expected.  negative\n",
            "8            Good value for money. Pretty satisfied.  positive\n",
            "9           Horrible experience. Never buying again.  negative\n",
            "\n",
            "Dataset shape: (10, 2)\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    4\n",
            "negative    4\n",
            "neutral     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Text Preprocessing for Reviews"
      ],
      "metadata": {
        "id": "aouglDbW_UNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_review(text):\n",
        "    \"\"\"Preprocess review text\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_review'] = df['review'].apply(preprocess_review)\n",
        "\n",
        "print(\"\\nBefore Processing:\")\n",
        "print(df['review'].iloc[0])\n",
        "print(\"\\nAfter Processing:\")\n",
        "print(df['processed_review'].iloc[0])"
      ],
      "metadata": {
        "id": "PDogr7se20Ei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d92387-c15a-4c9c-d587-ed1e4e24eb10"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before Processing:\n",
            "I absolutely love this product! Best purchase ever!\n",
            "\n",
            "After Processing:\n",
            "absolutely love product best purchase ever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Feature Extraction\n",
        "\n",
        "**What is Feature Extraction?**\n",
        "Text is unstructured. ML models need numbers. Feature extraction converts text into numerical features.\n",
        "\n",
        "**Method 1: Bag of Words (BoW)**"
      ],
      "metadata": {
        "id": "UrSM8QW1ABcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create BoW vectors\n",
        "bow_vectorizer = CountVectorizer(max_features=20)\n",
        "bow_features = bow_vectorizer.fit_transform(df['processed_review'])\n",
        "\n",
        "print(\"Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
        "print(\"Feature shape:\", bow_features.shape)\n",
        "print(\"Sample features (first review):\")\n",
        "print(bow_features[0].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfyiZS8tAEbD",
        "outputId": "5089c159-f6e5-4dac-c78d-209cdfb2e2ae"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['absolutely' 'amazing' 'awful' 'bad' 'best' 'broke' 'completely'\n",
            " 'customer' 'day' 'definitely' 'delivery' 'disappointed' 'ever' 'expected'\n",
            " 'experience' 'fantastic' 'fast' 'money' 'product' 'quality']\n",
            "Feature shape: (10, 20)\n",
            "Sample features (first review):\n",
            "[[1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**: Bag of Words counts occurrences of each word. Output is a matrix where each column represents a word, and each row represents a document.\n",
        "\n",
        "**Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)**"
      ],
      "metadata": {
        "id": "9ybxo4xm29X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TF-IDF vectors\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['processed_review'])\n",
        "\n",
        "print(\"Feature shape:\", tfidf_features.shape)\n",
        "print(\"Sample TF-IDF weights (first review):\")\n",
        "print(tfidf_features[0].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BBGIciGAKxx",
        "outputId": "45d36f39-5ad1-4b2c-b64c-3040e93def3c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (10, 20)\n",
            "Sample TF-IDF weights (first review):\n",
            "[[0.5182909  0.         0.         0.         0.5182909  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.5182909  0.         0.         0.         0.         0.\n",
            "  0.44059462 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why TF-IDF Better Than BoW**?\n",
        "- **TF**: How often a word appears in a document\n",
        "- **IDF**: How unique a word is across all documents\n",
        "- **Result**: Common words get lower weights, rare meaningful words get higher weights"
      ],
      "metadata": {
        "id": "FbZlOE4VAOqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Build Sentiment Classifier\n"
      ],
      "metadata": {
        "id": "KBIetFdHATHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For larger datasets, use train-test split\n",
        "# For now, we'll train on full dataset for demonstration\n",
        "\n",
        "# Create features using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=30, stop_words='english')\n",
        "X = tfidf.fit_transform(df['processed_review'])\n",
        "y = df['sentiment']\n",
        "\n",
        "# Create and train classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(X)\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"ðŸ“Š MODEL PERFORMANCE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y, y_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OajPiy44AXHq",
        "outputId": "8fd22734-90f5-4e5a-c156-1189d6f385ed"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š MODEL PERFORMANCE\n",
            "==================================================\n",
            "Accuracy:  1.0000\n",
            "Precision: 1.0000\n",
            "Recall:    1.0000\n",
            "F1-Score:  1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4 0 0]\n",
            " [0 2 0]\n",
            " [0 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Metrics**:\n",
        "- **Accuracy**: % of correct predictions overall\n",
        "- **Precision**: Of positive predictions, how many were correct?\n",
        "- **Recall**: Of actual positives, how many did we catch?\n",
        "- **F1-Score**: Harmonic mean of precision and recall"
      ],
      "metadata": {
        "id": "Eh15PS2T3A0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Test on New Reviews"
      ],
      "metadata": {
        "id": "AFReAgccAeIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    \"\"\"Predict sentiment of new text\"\"\"\n",
        "    # Preprocess\n",
        "    processed = preprocess_review(text)\n",
        "\n",
        "    # Vectorize\n",
        "    features = tfidf.transform([processed])\n",
        "\n",
        "    # Predict\n",
        "    prediction = classifier.predict(features)[0]\n",
        "    confidence = max(classifier.predict_proba(features)[0])\n",
        "\n",
        "    return prediction, confidence\n",
        "\n",
        "# Test on new reviews\n",
        "test_reviews = [\n",
        "    \"This product is fantastic! Love it!\",\n",
        "    \"Absolutely horrible. Do not buy.\",\n",
        "    \"It's alright, nothing special.\"\n",
        "]\n",
        "\n",
        "print(\"\\n TESTING ON NEW REVIEWS\")\n",
        "print(\"=\"*50)\n",
        "for review in test_reviews:\n",
        "    sentiment, confidence = predict_sentiment(review)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.2%})\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epf4QgnpAjzg",
        "outputId": "1def94a0-7f45-4606-94d9-a3ca0380c4e5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TESTING ON NEW REVIEWS\n",
            "==================================================\n",
            "Review: This product is fantastic! Love it!\n",
            "Sentiment: positive (Confidence: 51.97%)\n",
            "\n",
            "Review: Absolutely horrible. Do not buy.\n",
            "Sentiment: positive (Confidence: 45.36%)\n",
            "\n",
            "Review: It's alright, nothing special.\n",
            "Sentiment: negative (Confidence: 40.00%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Advanced Concepts"
      ],
      "metadata": {
        "id": "r-DeDpmsAp66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Named Entity Recognition (NER)\n",
        "\n",
        "**Definition**: Identifying and classifying named entities in text (persons, organizations, locations, etc.)\n"
      ],
      "metadata": {
        "id": "MEEVPM5EAtWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Apple CEO Tim Cook announced new products in San Francisco on January 12, 2026.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "print(\"-\" * 50)\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:20} â†’ {ent.label_:15} ({spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nAf5tQoAyGe",
        "outputId": "2805b8d0-a40e-4f9d-bfca-28f4c96481a0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "--------------------------------------------------\n",
            "Apple                â†’ ORG             (Companies, agencies, institutions, etc.)\n",
            "Tim Cook             â†’ PERSON          (People, including fictional)\n",
            "San Francisco        â†’ GPE             (Countries, cities, states)\n",
            "January 12, 2026     â†’ DATE            (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Part-of-Speech (POS) Tagging\n",
        "\n",
        "**Definition**: Assigning grammatical roles to words (noun, verb, adjective, etc.)\n"
      ],
      "metadata": {
        "id": "z8eak7Cr3Msp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cat quickly jumped over the fence.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"POS Tagging:\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:15} â†’ {token.pos_:10} ({token.tag_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEaYaYc7A460",
        "outputId": "cd46d868-7192-4750-f22f-4825533206c4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging:\n",
            "--------------------------------------------------\n",
            "The             â†’ DET        (DT)\n",
            "cat             â†’ NOUN       (NN)\n",
            "quickly         â†’ ADV        (RB)\n",
            "jumped          â†’ VERB       (VBD)\n",
            "over            â†’ ADP        (IN)\n",
            "the             â†’ DET        (DT)\n",
            "fence           â†’ NOUN       (NN)\n",
            ".               â†’ PUNCT      (.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Dependency Parsing\n",
        "\n",
        "**Definition**: Understanding grammatical relationships between words"
      ],
      "metadata": {
        "id": "5_wsRwBfA9Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Dependency Parsing:\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:15} â†’ {token.dep_:10} (head: {token.head.text})\")\n",
        "\n",
        "# Visualization\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "r4cRdpHfBBmy",
        "outputId": "0be02af2-ddbf-474d-93aa-c589e21d5dce"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing:\n",
            "--------------------------------------------------\n",
            "The             â†’ det        (head: fox)\n",
            "quick           â†’ amod       (head: fox)\n",
            "brown           â†’ amod       (head: fox)\n",
            "fox             â†’ nsubj      (head: jumps)\n",
            "jumps           â†’ ROOT       (head: jumps)\n",
            "over            â†’ prep       (head: jumps)\n",
            "the             â†’ det        (head: dog)\n",
            "lazy            â†’ amod       (head: dog)\n",
            "dog             â†’ pobj       (head: over)\n",
            ".               â†’ punct      (head: jumps)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"98f3135ee2444a428bcb95d6d1c7afad-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-98f3135ee2444a428bcb95d6d1c7afad-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-98f3135ee2444a428bcb95d6d1c7afad-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4 Word Embeddings Introduction\n",
        "\n",
        "**Definition**: Representing words as dense vectors in continuous space where similar words are close together\n",
        "\n",
        "**Concept**: Instead of one-hot encoding (sparse), use embeddings (dense, semantic meaning)\n"
      ],
      "metadata": {
        "id": "bcF8ehVS3SYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using spaCy word vectors\n",
        "text = \"The quick brown fox\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Word Vectors (first 5 dimensions):\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:15} â†’ {token.vector[:5]}\")\n",
        "\n",
        "# Similarity computation\n",
        "word1 = nlp(\"dog\")\n",
        "word2 = nlp(\"cat\")\n",
        "word3 = nlp(\"apple\")\n",
        "\n",
        "print(f\"\\nSimilarity Scores:\")\n",
        "print(f\"dog - cat: {word1.similarity(word2):.4f} (similar)\")\n",
        "print(f\"dog - apple: {word1.similarity(word3):.4f} (different)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qna88-_DBM48",
        "outputId": "0fa36532-af65-42ff-fbc5-5483e9b5da38"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vectors (first 5 dimensions):\n",
            "--------------------------------------------------\n",
            "The             â†’ [ 0.44080257  0.01434215  0.23331341  0.6461768  -0.27183467]\n",
            "quick           â†’ [ 1.072269   -0.88797593  0.9165765   0.5815992  -0.8369408 ]\n",
            "brown           â†’ [ 0.5068223  -0.86238426  0.88640505  0.15510865 -0.683393  ]\n",
            "fox             â†’ [ 0.04414169 -0.54116184 -0.319565    0.6428287   0.30003244]\n",
            "\n",
            "Similarity Scores:\n",
            "dog - cat: 0.7423 (similar)\n",
            "dog - apple: 0.6848 (different)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2404736424.py:16: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f\"dog - cat: {word1.similarity(word2):.4f} (similar)\")\n",
            "/tmp/ipython-input-2404736424.py:17: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f\"dog - apple: {word1.similarity(word3):.4f} (different)\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Complete Project Workflow\n"
      ],
      "metadata": {
        "id": "uQJXU5i53W7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE SENTIMENT ANALYSIS PIPELINE\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re\n",
        "\n",
        "# Step 1: Prepare Data\n",
        "reviews_data = {\n",
        "    'text': [\n",
        "        \"This is the best product I've ever used!\",\n",
        "        \"Terrible quality, completely disappointed.\",\n",
        "        \"Average product, nothing special.\",\n",
        "        \"Absolutely amazing! Highly recommended!\",\n",
        "        \"Waste of money. Broke after one week.\",\n",
        "        \"Decent value for the price.\",\n",
        "    ],\n",
        "    'label': ['positive', 'negative', 'neutral', 'positive', 'negative', 'neutral']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(reviews_data)\n",
        "\n",
        "# Step 2: Create Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(lowercase=True, stop_words='english', max_features=50)),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Step 3: Train\n",
        "pipeline.fit(df['text'], df['label'])\n",
        "\n",
        "# Step 4: Predict\n",
        "test_text = \"This product is absolutely wonderful!\"\n",
        "prediction = pipeline.predict([test_text])[0]\n",
        "confidence = max(pipeline.predict_proba([test_text])[0])\n",
        "\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Sentiment: {prediction}\")\n",
        "print(f\"Confidence: {confidence:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dma7p_ETBT-Z",
        "outputId": "d0be98c4-9af1-4097-f26a-ac1c64285508"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This product is absolutely wonderful!\n",
            "Sentiment: positive\n",
            "Confidence: 42.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **Tokenization** breaks text into meaningful units\n",
        "- **Preprocessing** (cleaning, lemmatization) improves model performance\n",
        "- **Feature Extraction** (BoW, TF-IDF) converts text to numbers\n",
        "- **Stop Words** often removed to improve signal-to-noise ratio\n",
        "- **Lemmatization** better than stemming for accuracy (but slower)\n",
        "- **TF-IDF** better than BoW for representing important words\n",
        "- **NER** identifies entities like people, places, organizations\n",
        "- **Word embeddings** capture semantic meaning\n",
        "\n",
        "---\n",
        "\n",
        "## Practice Exercises\n",
        "\n",
        "1. **Exercise 1**: Build a sentiment classifier on your own dataset\n",
        "2. **Exercise 2**: Implement a spam detection system using NLP\n",
        "3. **Exercise 3**: Create a text summarization tool\n",
        "4. **Exercise 4**: Build a simple chatbot using NLP techniques\n",
        "5. **Exercise 5**: Perform NER on news articles\n",
        "\n",
        "---\n",
        "\n",
        "## Further Learning Resources\n",
        "\n",
        "- **Books**: \"Natural Language Processing with Python\" (NLTK Book)\n",
        "- **Libraries**: spaCy, NLTK, TextBlob, Hugging Face Transformers\n",
        "- **Advanced Topics**: BERT, GPT, Transformers, Transfer Learning\n",
        "- **Datasets**: Movie Reviews, Restaurant Reviews, Tweet Sentiment datasets\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2xSR6Dxm1Zid"
      }
    }
  ]
}